{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Disciplined-22/gpt_2train_1/blob/main/gpt2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDiH84NtRdXb",
        "outputId": "9094bfee-a1f9-47a1-80b9-e70d6801eaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIjN_NvyTBl5"
      },
      "outputs": [],
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRkaiaZnRBK8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import accelerate\n",
        "import transformers\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbFjEAxZSS1m"
      },
      "outputs": [],
      "source": [
        "#function to read text file\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/gpt_1/gpt_sub_1/neural.txt\"  #The actual file path\n",
        "text = read_txt(file_path)\n",
        "print(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0FQHbnAY-Bd"
      },
      "outputs": [],
      "source": [
        "#code writes the content of the text variable into a file located in the directory\n",
        "with open(\"/content/drive/MyDrive/gpt_1/gpt_sub_1/neural.txt\", \"w\") as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sLviCgDZmAP"
      },
      "outputs": [],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vufY6M4Zpoc"
      },
      "outputs": [],
      "source": [
        "#funtion to load data in order to feed data to model\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Failed way for using  dataloader from pytorch don't run this code\n",
        "\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('text', data_files= file_path)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Encode the text and also return the labels\n",
        "  return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "  # Tokenize your dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "  # Convert to PyTorch DataLoader\n",
        "dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, collate_fn=data_collator)\n"
      ],
      "metadata": {
        "id": "4zKuP6B7OXF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "047ygt8Wbe1p"
      },
      "outputs": [],
      "source": [
        "# Function to load data from a file and prepare it for model input.\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "3z6eM1-j2ZhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZqoQKuVbkHr"
      },
      "outputs": [],
      "source": [
        "# This function trains a GPT2 model on a given dataset.\n",
        "#\n",
        "# Parameters:\n",
        "# train_file_path (str): The path to the training data file.\n",
        "# model_name (str): The name of the pre-trained GPT2 model to use.\n",
        "# output_dir (str): The directory where the trained model and tokenizer will be saved.\n",
        "# overwrite_output_dir (bool): If True, overwrite the content of the output directory.\n",
        "# per_device_train_batch_size (int): The batch size for training.\n",
        "# num_train_epochs (int): The number of training epochs.\n",
        "# save_steps (int): The number of steps between each checkpoint save.\n",
        "def train(train_file_path, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps):\n",
        "  # Load the tokenizer from the pre-trained GPT2 model\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "  # Load the dataset using the tokenizer\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "\n",
        "  # Load the data collator\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  # Save the tokenizer to the output directory\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  # Load the model from the pre-trained GPT2 model\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  # Save the model to the output directory\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  # Define the training arguments\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  # Initialize the trainer with the model, training arguments, data collator, and training dataset\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  trainer.train()\n",
        "\n",
        "  # Save the trained model\n",
        "  trainer.save_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfLqzEGmohVG"
      },
      "outputs": [],
      "source": [
        "file_path = file_path\n",
        "model_name = 'gpt2'\n",
        "output_dir = '/content/drive/MyDrive/gpt_1/gpt_sub_2'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 50.0\n",
        "save_steps = 50000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2MOXDdJp8l6"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "train(\n",
        "    train_file_path=file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-bMQ6aCqWer"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cW1mlHaqZ64"
      },
      "outputs": [],
      "source": [
        "# Function to load a pre-trained GPT2 model from a given path.\n",
        "# Parameters:\n",
        "# model_path (str): The path to the pre-trained model.\n",
        "# Returns:\n",
        "# model: The loaded GPT2 model.\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "# Function to load a pre-trained GPT2 tokenizer from a given path.\n",
        "# Parameters:\n",
        "# tokenizer_path (str): The path to the pre-trained tokenizer.\n",
        "# Returns:\n",
        "# tokenizer: The loaded GPT2 tokenizer.\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "# Function to generate text using a pre-trained GPT2 model.\n",
        "# Parameters:\n",
        "# model_path (str): The path to the pre-trained model.\n",
        "# sequence (str): The initial sequence to start the text generation.\n",
        "# max_length (int): The maximum length of the generated text.\n",
        "def generate_text(model_path, sequence, max_length):\n",
        "    # Load the model and tokenizer from the given path\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "\n",
        "    # Encode the initial sequence into tokens\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "\n",
        "    # Generate text from the initial sequence of tokens\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens into text and print it\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iusNFCuMst-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188f43e9-b78c-4955-e01b-d3b1ba4237eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q]what is neural net?\n",
            "Neural networks are computational systems that learn from data by manipulating its properties and patterns. Their architecture involves layers that connect nodes to a computer, serving as hubs for connections. The size and complexity of these networks make them suitable for training purposes, such as in games and natural language processing.\n",
            "Conclusions and Future Directions:\n",
            "Despite their success, neural nets are not without challenges. Recent advancements, such as attention mechanisms, latent processes, and reinforcement learning,\n"
          ]
        }
      ],
      "source": [
        "model2_path = \"/content/drive/MyDrive/gpt_1/gpt_sub_2\"\n",
        "sequence2 = \"[Q]what is neural net ?\"\n",
        "max_len = 100\n",
        "generate_text(model2_path, sequence2, max_len)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QLVJq7DLLy8CQ7v1XOdb6_JDs1mcTUhg",
      "authorship_tag": "ABX9TyPGUKuR5kWH8uX+DSId+fLF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}